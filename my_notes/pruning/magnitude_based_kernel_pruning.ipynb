{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=10>**Magnitude-based Kernel Pruning**</font>\n",
    "\n",
    "+ 构建、训练lenet网络\n",
    "+ 以lenet为例介绍基于权重幅值的剪枝基本流程\n",
    "+ 训练剪枝后的模型\n",
    "+ 比较模型剪枝前后的推理速度、精度差异\n",
    "+ 比较量化后的预训练模型和量化后的剪枝训练后的模型的推理速度、精度差异(本文不描述了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf verion = 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"tf verion = {tf.__version__}\")\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.layers import InputLayer,Reshape,Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*解决GPU内存不足报错，对GPU进行按需分配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小工具：查看一下各层的saprsity**\n",
    "\n",
    "只有两个conv和三个dense层有weights，其中dense层还有biases\n",
    "通过打印出的结果可以看到:\n",
    "+ 所有的bias都没有剪枝\n",
    "+ 剪枝是按层进行的，每层都剪掉了60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(weights):\n",
    "    return 1.0 - np.count_nonzero(weights) / float(weights.size)\n",
    "\n",
    "def list_sparsity(_model):\n",
    "    for layer in _model.layers:\n",
    "        for weight in layer.get_weights():\n",
    "            '''\n",
    "            print(np.allclose(\n",
    "                target_sparsity, get_sparsity(tf.keras.backend.get_value(weight)), \n",
    "                rtol=1e-6, atol=1e-6)\n",
    "            )\n",
    "            '''\n",
    "            print('%s sparsity:%f' %(layer.name, get_sparsity(tf.keras.backend.get_value(weight))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建、训练Lenet网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST 数据集\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 归一化输入图片，这样每个像素的值都在[0, 1]之间\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 扩张输入数据维度[height, width, channels(depth)]\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建LeNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 model:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         150       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               94200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 107,764\n",
      "Trainable params: 107,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        Conv2D(filters=6,kernel_size=5,strides=(1,1),padding='same',activation='relu',use_bias=False,input_shape=(28,28,1)),\n",
    "        MaxPool2D(pool_size=(3,3),strides=2,padding=\"same\"),\n",
    "        Conv2D(filters=16,kernel_size=5,strides=(1,1),padding='same',activation='relu',use_bias=False),\n",
    "        MaxPool2D(pool_size=(3,3),strides=2,padding=\"same\"),\n",
    "        Flatten(input_shape=(7, 7)),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "print(\"float32 model:\")\n",
    "model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> training\n",
      "1688/1688 [==============================] - 96s 57ms/step - loss: 1.5909 - accuracy: 0.8744 - val_loss: 1.4972 - val_accuracy: 0.9648\n",
      "==> evaluate\n",
      "313/313 - 2s - loss: 1.4998 - accuracy: 0.9621\n"
     ]
    }
   ],
   "source": [
    "print(\"==> training\")\n",
    "model.fit(x_train, y_train, epochs=1, validation_split=0.1)\n",
    "print(\"==> evaluate\")\n",
    "baseline_model_accuracy = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/lenet_normal.hdf5\")\n",
    "model_json = model.to_json()\n",
    "with open('./model/lenet_normal.json', 'w') as file:\n",
    "    file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络剪枝(magnitude-base pruning)&Fine-tune\n",
    "开始修剪掉10%，最终修剪掉60%(60%的dense layer kernel的weights和bias为0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 剪枝方式是自定义的，渐进的修剪，不要修剪的太频繁，在修剪过程中给模型留出修剪后恢复精度的充裕时间\n",
    "+ 只修剪dense层，不修剪分类头\n",
    "\n",
    "## 剪枝是如何渐进进行的\n",
    "设定起始剪枝sparsity和最终剪枝sparsity，譬如0.1~0.6。将整个剪枝过程分为11个（可自定义）[0.1, 0.15, 0.2, ..., 0.6]，将训练过程的所有batch分为16段（可自定义，但要大于剪枝的段数）。只在trainning阶段进行剪枝。在每段batch中的每个train_batch中使用相同的剪枝sparsity（每个train_batch都需要剪枝）。对于具体的每一个train_batch，需要在batch_end的时候剪枝，这样才能保证在每个train_batch结束后sparsity是我们期望的。设想假如我们在train_batch_begin时剪枝，剪枝后经过这个batch的训练，权重会发生变化，某些被我们修剪为0的权重可能变为非零值。\n",
    "\n",
    "举例说明：\n",
    "batch segment:      [0, 10, 20, 30, ..., 150, 160] 共16段\n",
    "sparsity segment:   [0.1, 0.15, 0.2, 0.25, ..., 0.6] 共11个\n",
    "\n",
    "+ 在train_batch [10, 20]之间的每个batch我们的剪枝sparsity都是0.15\n",
    "+ [90, 100]之间的每个batch剪枝sparsity都是0.55\n",
    "+ [100, 160]之间的每个batch我们的剪枝saprsity都是0.6\n",
    "+ 在我们进行最后一个sparsity 0.6的剪枝时，我们当然是希望能够用尽量多一点的batch去训练它，以期望得到一个较好的训练效果\n",
    "+ 在每个train_batch_end时剪枝，考虑到最后一次剪枝，为了保证sparsity使我们期望的，那么不能在剪枝后再训练，所以选择在train_batch_end时剪枝\n",
    "\n",
    "## 剪枝哪些权重\n",
    "我们构建的剪枝工具只对dense layer进行剪枝，对每个神经元的weights权重列向量按照L2范数进行排序，按照sparsity将最小的那部分weights列向量全部设置为0，这些weights列向量对应的bias值也设置为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载预训练网络，以此为基础进行剪枝，剪枝后的model_for_pruning引用了这一网络，会造成网络权重数据的覆盖，所以我们这边加载一个临时的网络用来剪枝，后续引用它时需要谨慎**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = load_model(\"./model/lenet_normal.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建剪枝网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         150       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               94200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 107,764\n",
      "Trainable params: 107,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1  # 10% of training set will be used for validation set.\n",
    "\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "\n",
    "def unit_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "    \"\"\"\n",
    "    Takes in matrices of kernel and bias weights (for a dense\n",
    "      layer) and returns the unit-pruned versions of each\n",
    "    Args:\n",
    "      k_weights: 2D matrix of the \n",
    "      b_weights: 1D matrix of the biases of a dense layer\n",
    "      k_sparsity: percentage of weights to set to 0\n",
    "    Returns:\n",
    "      kernel_weights: sparse matrix with same shape as the original\n",
    "        kernel weight matrix\n",
    "      bias_weights: sparse array with same shape as the original\n",
    "        bias array\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the kernel weights and get ranked indeces of the\n",
    "    # column-wise L2 Norms\n",
    "    kernel_weights = np.copy(k_weights)\n",
    "    ind = np.argsort(LA.norm(kernel_weights, axis=0))\n",
    "\n",
    "    # Number of indexes to set to 0\n",
    "    cutoff = int(len(ind) * k_sparsity)\n",
    "    # The indexes in the 2D kernel weight matrix to set to 0\n",
    "    sparse_cutoff_inds = ind[0:cutoff]\n",
    "    kernel_weights[:, sparse_cutoff_inds] = 0.\n",
    "\n",
    "    # Copy the bias weights and get ranked indeces of the abs\n",
    "    bias_weights = np.copy(b_weights)\n",
    "    # The indexes in the 1D bias weight matrix to set to 0\n",
    "    # Equal to the indexes of the columns that were removed in this case\n",
    "    #sparse_cutoff_inds\n",
    "    bias_weights[sparse_cutoff_inds] = 0.\n",
    "\n",
    "    return kernel_weights, bias_weights\n",
    "\n",
    "\n",
    "# Define model for pruning.\n",
    "class Prune_dense_layer(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, layer_index, start_batch, end_batch, start_sparsity,\n",
    "                 end_sparsity):\n",
    "        super(Prune_dense_layer, self).__init__()\n",
    "        self.layer_index = layer_index\n",
    "        self.global_batch = 0\n",
    "        self.idx = 0 \n",
    "        '''\n",
    "        这里将整个训练过程的batch划分为了16组，剪枝渐进进行，剪枝共划分为11组；\n",
    "        '''\n",
    "        self.prune_batch = np.arange(start_batch, end_batch,\n",
    "                                     int((end_batch - start_batch) / 16))\n",
    "        self.prune_sparsity = np.arange(start_sparsity, end_sparsity,\n",
    "                                        (end_sparsity - start_sparsity) / 10)\n",
    "        self.prune_sparsity = np.append(self.prune_sparsity, end_sparsity)\n",
    "        \n",
    "    def prune_dense_layer(self, sparsity):\n",
    "        for idx in self.layer_index:\n",
    "            layer = self.model.get_layer(index=idx)\n",
    "            new_weights = []\n",
    "            k_weights, k_bias= layer.get_weights()\n",
    "            k_weights_pruned, k_bias_pruned = unit_prune_dense_layer(\n",
    "                k_weights, k_bias, sparsity)\n",
    "            new_weights.append(k_weights_pruned)\n",
    "            new_weights.append(k_bias_pruned)\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        batch = self.global_batch\n",
    "        if batch in self.prune_batch:\n",
    "            idx = np.where(self.prune_batch == batch)\n",
    "            self.idx = idx[0].item()\n",
    "            \n",
    "        index = self.idx\n",
    "        if self.idx >= np.size(self.prune_sparsity):\n",
    "            index = np.size(self.prune_sparsity) - 1\n",
    "            \n",
    "        self.prune_dense_layer(sparsity=self.prune_sparsity[index])\n",
    "        '''\n",
    "        print(\"######prunning\")\n",
    "        print(\"batch %d\" %(self.prune_batch[index]))\n",
    "        print(\"sparsity %f\" %(self.prune_sparsity[index]))\n",
    "        '''\n",
    "            \n",
    "        self.global_batch = self.global_batch + 1\n",
    "\n",
    "\n",
    "prune_dense_layer_callback = Prune_dense_layer(layer_index=[5, 6],\n",
    "                                               start_batch=0,\n",
    "                                               end_batch=end_step,\n",
    "                                               start_sparsity=0.1,\n",
    "                                               end_sparsity=0.6)\n",
    "\n",
    "# 只修剪我们指定的层\n",
    "model_for_pruning = tf.keras.models.clone_model(pretrained_model)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune剪枝网络&评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> training\n",
      "Epoch 1/2\n",
      "422/422 [==============================] - 42s 101ms/step - loss: 1.6497 - accuracy: 0.8279 - val_loss: 1.5058 - val_accuracy: 0.9627\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 40s 95ms/step - loss: 1.5228 - accuracy: 0.9467 - val_loss: 1.4866 - val_accuracy: 0.9770\n",
      "==> evaluate\n",
      "313/313 - 2s - loss: 1.4911 - accuracy: 0.9723\n"
     ]
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "callbacks = [\n",
    "    prune_dense_layer_callback\n",
    "]\n",
    "\n",
    "print(\"==> training\")\n",
    "model_for_pruning.fit(x_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "print(\"==> evaluate\")\n",
    "model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看一下剪枝strip后模型各层的sparsity\n",
    "\n",
    "只有两个conv和三个dense层有weights，其中dense层还有biases\n",
    "通过打印出的结果可以看到:\n",
    "+ 剪枝是按层进行的，每层都剪掉了60%\n",
    "\n",
    "这里dense_1层的sparsity不是0.6是因为这一层有84个神经元，86 * 0.6 = 50.4不是整数，我们只能修剪掉50个神经元，50 / 84 = 0.595238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d sparsity:0.000000\n",
      "conv2d_1 sparsity:0.000000\n",
      "dense sparsity:0.600000\n",
      "dense sparsity:0.600000\n",
      "dense_1 sparsity:0.595238\n",
      "dense_1 sparsity:0.595238\n",
      "dense_2 sparsity:0.000000\n",
      "dense_2 sparsity:0.000000\n"
     ]
    }
   ],
   "source": [
    "list_sparsity(model_for_pruning)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存strip后的Fine-tune模型\n",
    "+ 模型的压缩率得到了提升：\n",
    "由于有60%的权重被我们剪掉了(值为0)，因此模型压缩后可以获得更小的体积\n",
    "采用\"bz2\"压缩，压缩前预训练模型和剪枝后模型大小都是1338456bytes，压缩后分别为1239917bytes和969408bytes。可以看出剪枝后模型小了很多。\n",
    "+ 网络的运行时间得到了减少："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.save(\"./model/lenet_prune.hdf5\")\n",
    "model_for_pruning_json = model_for_pruning.to_json()\n",
    "with open('./model/lenet_prune.json', 'w') as file:\n",
    "    file.write(model_for_pruning_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化Fine-tune模型\n",
    "+ 将预训练模型和Fine-tune模型转换为TFLite格式\n",
    "+ 量化模型\n",
    "+ 比较剪枝前后的模型在量化后的精度差异\n",
    "+ 查看量化后的Fine-tune模型的sparsity是否还是60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换和量化：预训练模型和Fine-tune模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert TFLite done\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train.astype(np.float32)).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "        yield [input_value]\n",
    "\n",
    "# 将原始keras模型转换为tflite模型，并执行量化（float fallback quantization, tf2.3之后才支持full integer quant）\n",
    "base_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "base_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "base_converter.representative_dataset = representative_data_gen\n",
    "base_tflite_model = base_converter.convert()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_pruning)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "print('convert TFLite done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存TFLite训练后量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/lenet_normal.tflite', 'wb') as file:\n",
    "    file.write(base_tflite_model)\n",
    "with open('./model/lenet_prune.tflite', 'wb') as file:\n",
    "    file.write(pruned_tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估预训练模型和Fine-tune模型量化后的精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quantized TFLite model accuracy compare:===>\n",
      "Base(pre-trained) TFLite test_accuracy: 0.9623\n",
      "Pruned TFLite test_accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(x_test):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == y_test).mean()\n",
    "    return accuracy\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=base_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "prune_interpreter = tf.lite.Interpreter(model_content=pruned_tflite_model)\n",
    "prune_interpreter.allocate_tensors()\n",
    "prune_test_accuracy = evaluate_model(prune_interpreter)\n",
    "\n",
    "print(\"Quantized TFLite model accuracy compare:===>\")\n",
    "print('Base(pre-trained) TFLite test_accuracy:', test_accuracy)\n",
    "print('Pruned TFLite test_accuracy:', prune_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看剪枝训练量化后模型各层的信息，从而找出权重所在层的index，便于我们提取权重**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_input_int8\n",
      "[ 1 28 28  1]\n",
      "(0.003921568859368563, -128)\n",
      "0\n",
      "sequential/dense/BiasAdd/ReadVariableOp\n",
      "[120]\n",
      "(8.208009239751846e-05, 0)\n",
      "1\n",
      "sequential/dense_1/BiasAdd/ReadVariableOp\n",
      "[84]\n",
      "(0.00020712010154966265, 0)\n",
      "2\n",
      "sequential/dense_2/BiasAdd/ReadVariableOp\n",
      "[10]\n",
      "(0.0003023018944077194, 0)\n",
      "3\n",
      "sequential/flatten/Const\n",
      "[2]\n",
      "(0.0, 0)\n",
      "4\n",
      "sequential/dense/MatMul\n",
      "[120 784]\n",
      "(0.0029068407602608204, 0)\n",
      "5\n",
      "sequential/dense_1/MatMul\n",
      "[ 84 120]\n",
      "(0.002946030581369996, 0)\n",
      "6\n",
      "sequential/dense_2/MatMul\n",
      "[10 84]\n",
      "(0.0033457628451287746, 0)\n",
      "7\n",
      "sequential/conv2d/Conv2D\n",
      "[6]\n",
      "(0.0, 0)\n",
      "8\n",
      "sequential/conv2d/Conv2D1\n",
      "[6 5 5 1]\n",
      "(0.0, 0)\n",
      "9\n",
      "sequential/conv2d_1/Conv2D\n",
      "[16]\n",
      "(0.0, 0)\n",
      "10\n",
      "sequential/conv2d_1/Conv2D1\n",
      "[16  5  5  6]\n",
      "(0.0, 0)\n",
      "11\n",
      "sequential/conv2d/Relu;sequential/conv2d/Conv2D\n",
      "[ 1 28 28  6]\n",
      "(0.011796954087913036, -128)\n",
      "12\n",
      "sequential/max_pooling2d/MaxPool\n",
      "[ 1 14 14  6]\n",
      "(0.011796954087913036, -128)\n",
      "13\n",
      "sequential/conv2d_1/Relu;sequential/conv2d_1/Conv2D\n",
      "[ 1 14 14 16]\n",
      "(0.02823687344789505, -128)\n",
      "14\n",
      "sequential/max_pooling2d_1/MaxPool\n",
      "[ 1  7  7 16]\n",
      "(0.02823687344789505, -128)\n",
      "15\n",
      "sequential/flatten/Reshape\n",
      "[  1 784]\n",
      "(0.02823687344789505, -128)\n",
      "16\n",
      "sequential/dense/Relu;sequential/dense/BiasAdd\n",
      "[  1 120]\n",
      "(0.07030480355024338, -128)\n",
      "17\n",
      "sequential/dense_1/Relu;sequential/dense_1/BiasAdd\n",
      "[ 1 84]\n",
      "(0.09035365283489227, -128)\n",
      "18\n",
      "sequential/dense_2/BiasAdd\n",
      "[ 1 10]\n",
      "(0.1872994303703308, -15)\n",
      "19\n",
      "Identity_int8\n",
      "[ 1 10]\n",
      "(0.00390625, -128)\n",
      "20\n",
      "conv2d_input\n",
      "[ 1 28 28  1]\n",
      "(0.0, 0)\n",
      "21\n",
      "Identity\n",
      "[ 1 10]\n",
      "(0.0, 0)\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "all_layer_details = prune_interpreter.get_tensor_details()\n",
    "for layer in all_layer_details:\n",
    "    print(layer['name'])\n",
    "    print(layer['shape'])\n",
    "    print(layer['quantization'])\n",
    "    print(layer['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计剪枝量化后模型的稀疏度(sparsity)\n",
    "+ 通过前面提取的信息可以知道权重所处的层，统计一下量化后权重的sparsity，结果显示量化没有改变sparsity，此处也即：量化后的权重层仍有80%的权重值是0.保持sparsity量化后不变的本质是：量化前0权重(float)量化为0权重(uint8)。根据我们的量化公式r=s(q - z)可知要想保证这一点，zero_point应该为0，进一步可以推导出min(r)应该为0，即权重的最小值必须是0。此处应该是特殊情况\n",
    "+ 目前没有发现有资料说明量化不改变sparsity\n",
    "+ 剪枝的使用应该更侧重于提高模型的可压缩性，便于在边缘设备上的部署。tensorflow给出的例子上也是只关注了模型的可压缩度（譬如lenet网络在剪枝后稀疏度为80%，通过zip压缩可以获得3倍的压缩效果，keras转换为tflite后再量化加上之前的剪枝一共可以获得10倍的压缩效果），没有关注模型的inference时间有无改善。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense sparsity:0.606420\n",
      "dense1 sparsity:0.598611\n",
      "dense2 sparsity:0.002381\n",
      "conv2d sparsity:0.006667\n",
      "conv2d_1 sparsity:0.009583\n"
     ]
    }
   ],
   "source": [
    "# layers: [5]:dense weights, [6]:dense1 weights, [7]:dense2 weights, [9]conv2d weights, [11]:conv2d_1 weights\n",
    "weight_layer_index = [5, 6 ,7 ,9 ,11]\n",
    "weight_layer_name = [\"dense\", \"dense1\" ,\"dense2\" ,\"conv2d\" ,\"conv2d_1\"]\n",
    "weight_layers = {\"dense\" : 5, \"dense1\" : 6,\"dense2\" : 7, \"conv2d\" : 9, \"conv2d_1\" : 11}\n",
    "\n",
    "for name, index in weight_layers.items():\n",
    "    weight = prune_interpreter.get_tensor(index)\n",
    "    print('%s sparsity:%f' %(name, get_sparsity(weight)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
