{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training\n",
    "+ 以lenet为例介绍量化感知训练的基本流程\n",
    "+ 部署量化感知训练后的网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf verion = 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"tf verion = {tf.__version__}\")\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.layers import InputLayer,Reshape,Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解决GPU内存不足报错，对GPU进行按需分配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST 数据集\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 归一化输入图片，这样每个像素的值都在[0, 1]之间\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 扩张输入数据维度[height, width, channels(depth)]\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建LeNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    LeNet5\n",
    "        Conv2D(filters=6,kernel_size=5,strides=(1,1),padding='same',activation='relu',use_bias=False,input_shape=(28,28,1)),\n",
    "        MaxPool2D(pool_size=(3,3),strides=2,padding=\"same\"),\n",
    "        Conv2D(filters=16,kernel_size=5,strides=(1,1),padding='same',activation='relu',use_bias=False),\n",
    "        MaxPool2D(pool_size=(3,3),strides=2,padding=\"same\"),\n",
    "        Flatten(input_shape=(7, 7)),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation='softmax')\n",
    "    '''\n",
    "model = tf.keras.models.Sequential([\n",
    "        Conv2D(filters=12, kernel_size=(3, 3),activation='relu',input_shape=(28, 28, 1)),\n",
    "        MaxPool2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和评估（普通方式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 model:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 12)        120       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,410\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "==> training\n",
      "1688/1688 [==============================] - 103s 61ms/step - loss: 0.3214 - accuracy: 0.9095 - val_loss: 0.1364 - val_accuracy: 0.9640\n",
      "==> evaluate\n",
      "313/313 - 2s - loss: 0.1600 - accuracy: 0.9544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15999484062194824, 0.9544000029563904]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"float32 model:\")\n",
    "model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(\"==> training\")\n",
    "model.fit(x_train, y_train, epochs=1, validation_split=0.1)\n",
    "print(\"==> evaluate\")\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lenet_normal.hdf5\")\n",
    "model_json = model.to_json()\n",
    "with open('lenet_normal.json', 'w') as file:\n",
    "    file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化感知训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quant_conv2d (QuantizeWrappe (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 13, 13, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 2028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 10)                20295     \n",
      "=================================================================\n",
      "Total params: 20,444\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 34\n",
      "_________________________________________________________________\n",
      "==> training\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.1809 - accuracy: 0.9511 - val_loss: 0.1957 - val_accuracy: 0.9400\n",
      "==> evaluate\n",
      "313/313 - 3s - loss: 2.8189 - accuracy: 0.3049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8189375400543213, 0.30489999055862427]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练模型\n",
    "pretrained_model = load_model(\"lenet_normal.hdf5\") \n",
    "\n",
    "# 量化感知训练\n",
    "print(\"quantized model:\")\n",
    "qat_model = tfmot.quantization.keras.quantize_model(pretrained_model)\n",
    "\n",
    "qat_model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "qat_model.summary()\n",
    "print(\"==> training\")\n",
    "x_train_subset = x_train[0:1000]\n",
    "y_train_subset = y_train[0:1000]\n",
    "qat_model.fit(x_train_subset, y_train_subset,\n",
    "              batch_size=500,\n",
    "              epochs=1,\n",
    "              validation_split=0.1)\n",
    "print(\"==> evaluate\")\n",
    "qat_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.save(\"lenet_qat.hdf5\")\n",
    "qat_model.save_weights(\"./weights/lenet_qat.hd5\")\n",
    "qat_model_json = qat_model.to_json()\n",
    "with open('lenet_qat.json', 'w') as file:\n",
    "    file.write(qat_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f35a4440310> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f36088dea60>).\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"./weights/lenet_qat.hd5\")\n",
    "model.save(\"lenet_qat_nofake_quant.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导出量化后的模型\n",
    "量化之前通过量化感知训练得到的模型，格式为TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-54eaacab334e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqat_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUANTIZED_UINT8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquantized_tflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lenet.tflite', 'wb') as file:\n",
    "    file.write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
